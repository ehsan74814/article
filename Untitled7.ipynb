{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pTmCjTN2Fd-4VqStalG8NzJ2_Tj3dqMv",
      "authorship_tag": "ABX9TyM2l+kohyb6kpkfR3YGumVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehsan74814/article/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets seqeval accelerate\n",
        ""
      ],
      "metadata": {
        "id": "9QWLpTaRUTpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "fABPqteNTRnr",
        "outputId": "c5688328-9483-4c65-8c8c-0f778d314f0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mtsamples_med_ie_conll.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-663314820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcurrent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconll_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mtsamples_med_ie_conll.txt'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "# =========================\n",
        "# 1. خواندن فایل CoNLL\n",
        "# =========================\n",
        "\n",
        "conll_file = \"mtsamples_med_ie_conll.txt\"  # اگر اسمش فرق دارد، اینجا عوض کن\n",
        "\n",
        "sentences_tokens = []\n",
        "sentences_labels = []\n",
        "\n",
        "current_tokens = []\n",
        "current_labels = []\n",
        "\n",
        "with open(conll_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            # پایان جمله\n",
        "            if current_tokens:\n",
        "                sentences_tokens.append(current_tokens)\n",
        "                sentences_labels.append(current_labels)\n",
        "                current_tokens = []\n",
        "                current_labels = []\n",
        "        else:\n",
        "            # هر خط: token \\t label\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            tok, lab = parts\n",
        "            current_tokens.append(tok)\n",
        "            current_labels.append(lab)\n",
        "\n",
        "# اگر فایل با خط خالی تمام نشده\n",
        "if current_tokens:\n",
        "    sentences_tokens.append(current_tokens)\n",
        "    sentences_labels.append(current_labels)\n",
        "\n",
        "print(f\"تعداد جملات خوانده شده: {len(sentences_tokens)}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2. ساخت لیست برچسب‌ها\n",
        "# =========================\n",
        "\n",
        "all_labels = sorted(list({lab for sent in sentences_labels for lab in sent}))\n",
        "print(\"لیست برچسب‌ها:\", all_labels)\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(all_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(\"label2id:\", label2id)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3. تقسیم train / val / test\n",
        "# =========================\n",
        "\n",
        "# برای reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    sentences_tokens, sentences_labels, test_size=0.1, random_state=random_seed\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1, random_state=random_seed\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4. آماده‌سازی توکنایزر و مدل\n",
        "# =========================\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"  # سبک و مناسب برای CPU / GPU سبک\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(all_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. ساخت Dataset کلاس\n",
        "# =========================\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, label2id, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # استفاده از encoding با word_ids برای align کردن برچسب‌ها\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=False,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # word_ids برای این است که بدانیم هر subtoken مربوط به کدام کلمه است\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "\n",
        "        aligned_labels = []\n",
        "        previous_word_id = None\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                # توکن‌های خاص [CLS], [SEP], ...\n",
        "                aligned_labels.append(-100)  # -100 باعث میشه در loss نادیده گرفته شود\n",
        "            else:\n",
        "                # برچسب کلمه\n",
        "                label_str = labels[word_id]\n",
        "                aligned_labels.append(self.label2id[label_str])\n",
        "\n",
        "        # تبدیل به tensor\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        encoding[\"labels\"] = torch.tensor(aligned_labels, dtype=torch.long)\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "train_dataset = NERDataset(X_train, y_train, tokenizer, label2id)\n",
        "val_dataset   = NERDataset(X_val,   y_val,   tokenizer, label2id)\n",
        "test_dataset  = NERDataset(X_test,  y_test,  tokenizer, label2id)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. تنظیمات Trainer\n",
        "# =========================\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    تابع محاسبه‌ی Precision, Recall, F1 با استفاده از seqeval\n",
        "    \"\"\"\n",
        "    logits, labels = pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_labels = []\n",
        "    true_preds = []\n",
        "\n",
        "    for pred_ids, label_ids in zip(predictions, labels):\n",
        "        cur_preds = []\n",
        "        cur_labels = []\n",
        "        for p, l in zip(pred_ids, label_ids):\n",
        "            if l == -100:\n",
        "                continue\n",
        "            cur_preds.append(id2label[p])\n",
        "            cur_labels.append(id2label[l])\n",
        "        true_labels.append(cur_labels)\n",
        "        true_preds.append(cur_preds)\n",
        "\n",
        "    precision = precision_score(true_labels, true_preds)\n",
        "    recall = recall_score(true_labels, true_preds)\n",
        "    f1 = f1_score(true_labels, true_preds)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./med_ner_distilbert\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,   # اگر GPU قوی نداری، همین بمونه\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,              # می‌تونی کم/زیادش کنی\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "#================================\n",
        "\n",
        "# =========================\n",
        "# 7. آموزش مدل\n",
        "# =========================\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 8. ارزیابی روی Test set\n",
        "# =========================\n",
        "\n",
        "print(\"ارزیابی روی داده‌ی تست...\")\n",
        "\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "pred_ids = np.argmax(predictions, axis=-1)\n",
        "\n",
        "true_labels = []\n",
        "true_preds = []\n",
        "\n",
        "for pred_seq, label_seq in zip(pred_ids, labels):\n",
        "    cur_preds = []\n",
        "    cur_labels = []\n",
        "    for p, l in zip(pred_seq, label_seq):\n",
        "        if l == -100:\n",
        "            continue\n",
        "        cur_preds.append(id2label[p])\n",
        "        cur_labels.append(id2label[l])\n",
        "    true_labels.append(cur_labels)\n",
        "    true_preds.append(cur_preds)\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(true_labels, true_preds))"
      ]
    }
  ]
}