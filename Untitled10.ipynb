{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BhoO-lEcRixkvhJt7H_sfc9hPcJXL-fO",
      "authorship_tag": "ABX9TyOdygz1mOH91w3TS46nOXRU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehsan74814/article/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# ================================\n",
        "# 1) لیست موجودیت‌ها\n",
        "# ================================\n",
        "DRUGS = [\n",
        "    \"metformin\", \"ibuprofen\", \"aspirin\", \"albuterol\", \"penicillin\",\n",
        "    \"lisinopril\", \"atorvastatin\", \"amoxicillin\", \"morphine\", \"prednisone\",\n",
        "]\n",
        "\n",
        "SYMPTOMS = [\n",
        "    \"headache\", \"nausea\", \"vomiting\", \"stomach pain\", \"dizziness\",\n",
        "    \"shortness of breath\", \"fatigue\", \"blurred vision\", \"rash\", \"swelling\",\n",
        "]\n",
        "\n",
        "PROBLEMS = [\n",
        "    \"diabetes\", \"hypertension\", \"asthma\", \"obesity\", \"infection\",\n",
        "    \"heart failure\", \"kidney disease\", \"migraine\", \"arthritis\", \"pneumonia\",\n",
        "]\n",
        "\n",
        "TESTS = [\n",
        "    \"elevated CRP\", \"abnormal ECG\", \"high blood sugar\", \"low oxygen level\",\n",
        "    \"elevated liver enzymes\", \"positive strep test\", \"abnormal X-ray\",\n",
        "]\n",
        "\n",
        "ADEs = [\n",
        "    \"skin rash\", \"liver damage\", \"internal bleeding\", \"severe allergy\",\n",
        "    \"anaphylaxis\", \"renal impairment\", \"giant hives\",\n",
        "]\n",
        "\n",
        "SOCIAL = [\n",
        "    \"smoking\", \"alcohol consumption\", \"drug abuse\", \"vaping\",\n",
        "]\n",
        "\n",
        "RELATIONS = [\n",
        "    \"treats\", \"causes\", \"adverse\", \"indicates\", \"neg\", \"interacts_with\",\n",
        "    \"improves\", \"worsens\"\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# 2)  ساخت جمله\n",
        "# ================================\n",
        "TEMPLATES = [\n",
        "    # Drug – Symptom\n",
        "    (\"{drug} often helps relieve [s2] {symptom} [e2].\",\n",
        "     \"The patient used [s1] {drug} [e1] yesterday.\",\n",
        "     \"treats\"),\n",
        "\n",
        "    # Drug – ADE\n",
        "    (\"Patient developed [s2] {ade} [e2] after taking medication.\",\n",
        "     \"He recently took [s1] {drug} [e1] for pain.\",\n",
        "     \"adverse\"),\n",
        "\n",
        "    # Drug – Problem (worsens)\n",
        "    (\"Using [s1] {drug} [e1] may worsen [s2] {problem} [e2].\",\n",
        "     \"Doctor noted the patient's chronic condition.\",\n",
        "     \"worsens\"),\n",
        "\n",
        "    # Test – Problem\n",
        "    (\"The report showed [s1] {test} [e1] values.\",\n",
        "     \"This finding may indicate [s2] {problem} [e2].\",\n",
        "     \"indicates\"),\n",
        "\n",
        "    # Drug – Symptom (causes)\n",
        "    (\"After using [s1] {drug} [e1], patient reported [s2] {symptom} [e2].\",\n",
        "     \"Side effects increased over time.\",\n",
        "     \"causes\"),\n",
        "\n",
        "    # Drug – Problem (treats)\n",
        "    (\"The physician prescribed [s1] {drug} [e1] for managing condition.\",\n",
        "     \"It helps control [s2] {problem} [e2].\",\n",
        "     \"treats\"),\n",
        "\n",
        "    # Social – Problem (neg)\n",
        "    (\"Patient denies history of [s1] {social} [e1].\",\n",
        "     \"Also denies any [s2] {problem} [e2] symptoms.\",\n",
        "     \"neg\"),\n",
        "\n",
        "    # Drug – Test (interacts_with)\n",
        "    (\"Use of [s1] {drug} [e1] can affect [s2] {test} [e2] readings.\",\n",
        "     \"Monitoring is required.\",\n",
        "     \"interacts_with\"),\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# 3)  دیتاست\n",
        "# ================================\n",
        "rows = []\n",
        "NUM_SAMPLES = 5000\n",
        "\n",
        "for i in range(NUM_SAMPLES):\n",
        "    template = random.choice(TEMPLATES)\n",
        "\n",
        "    sentence_1 = template[0].format(\n",
        "        drug=random.choice(DRUGS),\n",
        "        symptom=random.choice(SYMPTOMS),\n",
        "        problem=random.choice(PROBLEMS),\n",
        "        test=random.choice(TESTS),\n",
        "        ade=random.choice(ADEs),\n",
        "        social=random.choice(SOCIAL),\n",
        "    )\n",
        "\n",
        "    sentence_2 = template[1].format(\n",
        "        drug=random.choice(DRUGS),\n",
        "        symptom=random.choice(SYMPTOMS),\n",
        "        problem=random.choice(PROBLEMS),\n",
        "        test=random.choice(TESTS),\n",
        "        ade=random.choice(ADEs),\n",
        "        social=random.choice(SOCIAL),\n",
        "    )\n",
        "\n",
        "    relation_type = template[2]\n",
        "\n",
        "    # entity types بر اساس الگو\n",
        "    if \"drug\" in sentence_1 or \"drug\" in sentence_2:\n",
        "        entity_type_1 = \"Drug\"\n",
        "    elif \"test\" in sentence_1:\n",
        "        entity_type_1 = \"Test\"\n",
        "    else:\n",
        "        entity_type_1 = \"Other\"\n",
        "\n",
        "    # entity 2\n",
        "    if \"symptom\" in sentence_1 or \"symptom\" in sentence_2:\n",
        "        entity_type_2 = \"Symptom\"\n",
        "    elif \"problem\" in sentence_1 or \"problem\" in sentence_2:\n",
        "        entity_type_2 = \"Problem\"\n",
        "    elif \"ade\" in sentence_1:\n",
        "        entity_type_2 = \"ADE\"\n",
        "    else:\n",
        "        entity_type_2 = \"Other\"\n",
        "\n",
        "    rows.append({\n",
        "        \"relation_type\": relation_type,\n",
        "        \"sentence_1\": sentence_1,\n",
        "        \"sentence_2\": sentence_2,\n",
        "        \"entity_type_1\": entity_type_1,\n",
        "        \"entity_type_2\": entity_type_2,\n",
        "        \"entity_id_1\": f\"T{i*2+1}\",\n",
        "        \"entity_id_2\": f\"T{i*2+2}\",\n",
        "        \"file_id\": f\"file_{i:05d}\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"mtsamples_relations_5000.csv\", index=False)\n",
        "\n",
        "print(\" Synthetic dataset with 5000 samples created successfully!\")\n",
        "print(df.head())\n",
        ""
      ],
      "metadata": {
        "id": "S4esSiSliZcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  تنظیمات کلی\n",
        "# ==========================\n",
        "RANDOM_SEED = 13\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "DATA_CSV_PATH = \"mtsamples_relations_5000.csv\"\n",
        "\n",
        "# پوشه‌ای که خروجی‌ها (مدل‌ها + متریک‌ها) داخلش ذخیره می‌شود\n",
        "OUTPUT_ROOT = \"mtsamples_relation_experiments\"\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  1) خواندن CSV و ساخت train/dev/test\n",
        "# ==========================\n",
        "def load_and_split(csv_path: str, test_size: float = 0.2, dev_size: float = 0.1):\n",
        "    \"\"\"\n",
        "    CSV باید ستون‌های زیر را داشته باشد:\n",
        "    relation_type, sentence_1, sentence_2,\n",
        "    entity_type_1, entity_type_2, entity_id_1, entity_id_2, file_id\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    required_cols = [\n",
        "        \"relation_type\",\n",
        "        \"sentence_1\",\n",
        "        \"sentence_2\",\n",
        "        \"entity_type_1\",\n",
        "        \"entity_type_2\",\n",
        "        \"entity_id_1\",\n",
        "        \"entity_id_2\",\n",
        "        \"file_id\",\n",
        "    ]\n",
        "    for c in required_cols:\n",
        "        if c not in df.columns:\n",
        "            raise ValueError(\n",
        "                f\"ستون {c} داخل CSV پیدا نشد؛ \"\n",
        "                f\"اول باید این ستون‌ها را طبق فرمت n2c2 بسازی.\"\n",
        "            )\n",
        "\n",
        "    # Split مشابه چیزی که در کارهای n2c2 می‌کنند\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        stratify=df[\"relation_type\"],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "    train_df, dev_df = train_test_split(\n",
        "        train_df,\n",
        "        test_size=dev_size,\n",
        "        stratify=train_df[\"relation_type\"],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        train_df.reset_index(drop=True),\n",
        "        dev_df.reset_index(drop=True),\n",
        "        test_df.reset_index(drop=True),\n",
        "    )\n",
        "\n",
        "\n",
        "def save_tsv(df: pd.DataFrame, path: str):\n",
        "\n",
        "    df.to_csv(path, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  2) Dataset کلاس برای Trainer\n",
        "# ==========================\n",
        "@dataclass\n",
        "class RelationDataset(torch.utils.data.Dataset):\n",
        "    texts: List[str]\n",
        "    labels: List[int]\n",
        "    tokenizer: AutoTokenizer\n",
        "    max_len: int = 256\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "def build_dataset(df: pd.DataFrame, label2id: Dict[str, int], tokenizer, max_len: int = 256):\n",
        "    # مثل ریپو، دو جمله را کنار هم قرار می‌دهیم؛ اگر [s1]/[e1]/[s2]/[e2] داخل جمله‌ها هستند همان‌ها می‌مانند\n",
        "    texts = (df[\"sentence_1\"] + \" \" + df[\"sentence_2\"]).tolist()\n",
        "    labels = [label2id[y] for y in df[\"relation_type\"].tolist()]\n",
        "    return RelationDataset(texts=texts, labels=labels, tokenizer=tokenizer, max_len=max_len)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  3) متریک‌ها\n",
        "# ==========================\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    micro_f1 = f1_score(labels, preds, average=\"micro\")\n",
        "    macro_p = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "    macro_r = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"micro_f1\": micro_f1,\n",
        "        \"macro_precision\": macro_p,\n",
        "        \"macro_recall\": macro_r,\n",
        "    }\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  4) تعریف fast GELU برای GatorTron\n",
        "# ==========================\n",
        "def register_fast_gelu():\n",
        "    \"\"\"\n",
        "    تابع fast_gelu همان چیزی است که با ضریب 1.702 روی x * sigmoid کار می‌کند.\n",
        "    اینجا آن را به ACT2FN اضافه می‌کنیم تا config.hidden_act = \"gelu_fast\" کار کند.\n",
        "    \"\"\"\n",
        "    def fast_gelu(x):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "    ACT2FN[\"gelu_fast\"] = fast_gelu\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  5) لیست ۶ مدل\n",
        "# ==========================\n",
        "# name  |  (type فقط برای لاگ) |  checkpoint HF  |  آیا fast GELU فعال شود؟\n",
        "MODELS = [\n",
        "    (\"gatortron_fastgelu\", \"bert\", \"UFNLP/gatortron-base\", True),   # GatorTron تغییر یافته\n",
        "    (\"gatortron_base\", \"bert\", \"UFNLP/gatortron-base\", False),      # GatorTron اصلی\n",
        "    (\"roberta_mimic\", \"roberta\", \"mimiciii_roberta_10e_128b\", False),\n",
        "    (\"albert_mimic\", \"albert\", \"mimiciii_albert_10e_128b\", False),\n",
        "    (\"roberta_base\", \"roberta\", \"roberta-base\", False),\n",
        "    (\"albert_base\", \"albert\", \"albert-base-v2\", False),\n",
        "]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  6) حلقه اصلی برای اجرای همه مدل‌ها\n",
        "# ==========================\n",
        "def run_all_models():\n",
        "    # --- 6.1  خواندن CSV و split ---\n",
        "    train_df, dev_df, test_df = load_and_split(DATA_CSV_PATH)\n",
        "\n",
        "    # لیبل‌ها\n",
        "    label_list = sorted(train_df[\"relation_type\"].unique().tolist())\n",
        "    label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
        "    id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "\n",
        "    # اگر خواستی tsv مثل مقاله داشته باشی\n",
        "    save_tsv(train_df, os.path.join(OUTPUT_ROOT, \"train.tsv\"))\n",
        "    save_tsv(dev_df, os.path.join(OUTPUT_ROOT, \"dev.tsv\"))\n",
        "    save_tsv(test_df, os.path.join(OUTPUT_ROOT, \"test.tsv\"))\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # --- 6.2  برای هر مدل یکی یکی ---\n",
        "    for exp_name, model_type, ckpt, use_fast in MODELS:\n",
        "        print(f\"\\n====== Training model: {exp_name}  ({ckpt}) ======\")\n",
        "\n",
        "        exp_dir = os.path.join(OUTPUT_ROOT, exp_name)\n",
        "        os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "        # 6.2.1 فعال‌کردن fast GELU اگر لازم است\n",
        "        if use_fast:\n",
        "            register_fast_gelu()\n",
        "            config = AutoConfig.from_pretrained(\n",
        "                ckpt,\n",
        "                num_labels=len(label_list),\n",
        "            )\n",
        "            # اینجا همان تغییری است که در GatorTron می‌دهیم\n",
        "            config.hidden_act = \"gelu_fast\"\n",
        "        else:\n",
        "            config = AutoConfig.from_pretrained(\n",
        "                ckpt,\n",
        "                num_labels=len(label_list),\n",
        "            )\n",
        "\n",
        "        # 6.2.2 توکنایزر و مدل\n",
        "        tokenizer = AutoTokenizer.from_pretrained(ckpt, use_fast=True)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            ckpt,\n",
        "            config=config,\n",
        "            num_labels=len(label_list),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "        )\n",
        "\n",
        "        # 6.2.3 ساخت دیتاست‌ها\n",
        "        train_ds = build_dataset(train_df, label2id, tokenizer)\n",
        "        dev_ds = build_dataset(dev_df, label2id, tokenizer)\n",
        "        test_ds = build_dataset(test_df, label2id, tokenizer)\n",
        "\n",
        "        # 6.2.4 تنظیمات آموزش (تقریباً مطابق README)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=exp_dir,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=4,\n",
        "            num_train_epochs=3,\n",
        "            learning_rate=1e-5,\n",
        "            warmup_ratio=0.1,\n",
        "            weight_decay=0.0,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"macro_f1\",\n",
        "            greater_is_better=True,\n",
        "            seed=RANDOM_SEED,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=dev_ds,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        # 6.2.5 آموزش\n",
        "        trainer.train()\n",
        "\n",
        "        # 6.2.6 ارزیابی روی test\n",
        "        eval_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
        "        eval_metrics[\"model\"] = exp_name\n",
        "        all_results.append(eval_metrics)\n",
        "\n",
        "        # گزارش کامل برای هر مدل\n",
        "        preds = np.argmax(trainer.predict(test_ds).predictions, axis=-1)\n",
        "        y_true = test_df[\"relation_type\"].tolist()\n",
        "        y_pred = [id2label[p] for p in preds]\n",
        "\n",
        "        report = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            digits=4,\n",
        "        )\n",
        "        with open(os.path.join(exp_dir, \"classification_report.txt\"), \"w\") as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(report)\n",
        "\n",
        "    # 6.2.7 جمع‌بندی متریک‌ها در یک فایل\n",
        "    res_df = pd.DataFrame(all_results)\n",
        "    res_df.to_csv(os.path.join(OUTPUT_ROOT, \"summary_metrics.csv\"), index=False)\n",
        "    print(\"\\n✅ همه مدل‌ها تمام شدند؛ summary_metrics.csv ساخته شد.\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  اجرای مستقیم فایل\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_models()\n",
        ""
      ],
      "metadata": {
        "id": "Me8jAhGMZUs0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}